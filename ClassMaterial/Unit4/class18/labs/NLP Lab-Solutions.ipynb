{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Lab:  NLP\n",
    "\n",
    "Welcome to tonight's lab!  Tonight we're going to build a neural network to analyze text data.  We'll be using the IMDB dataset to train our model on movie reviews to predict whether or not they convery a positive or negative sentiment.  \n",
    "\n",
    "During the lab we'll use Keras to build a 3 layer neural network with word embeddings and densely connected outer layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1:  Read in the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "import pandas as pd\n",
    "df = pd.read_csv('IMDB.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Process Your Data\n",
    "\n",
    "Take the following steps:\n",
    "\n",
    " - For the target variable, encode `positive` and `negative` to `1` and `0`\n",
    " - Create a training and a test set.  Since there's no order to this dataset, randomly shuffling is fine.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "df.sentiment.replace(['positive', 'negative'], [1, 0], inplace=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3:  Tokenize Your Word Documents\n",
    "\n",
    "\n",
    "**3a:** Import the necessary portions of the keras library:\n",
    "\n",
    "To do this, you'll need the following parts of Keras:\n",
    "\n",
    " - `keras.preprocessing.text.Tokenizer`\n",
    " - `keras.preprocessing.sequences.pad_sequences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your answer here\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b:** Use the `Tokenizer` to process your text data.\n",
    "\n",
    "Use the following methods to appropriately process your training and test data:\n",
    "\n",
    " - `fit_on_texts`\n",
    " - `texts_to_sequences`\n",
    " \n",
    "**Note:** Use a maximum vocabulary size of 10000 words when you initialize the Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test  = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c:** Equalize the length of each review\n",
    "\n",
    "You have some discretion on this step, and you might want to play around with different variations of this if you have additional time, but for now set each document to 150 characters long, using the `pad_sequences` method in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "X_train = pad_sequences(X_train, maxlen=150)\n",
    "X_test  = pad_sequences(X_test, maxlen=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d:** Double check your data\n",
    "\n",
    "At this point, it's probably a good idea to make sure you understand what you just did, and how your data is setup.  \n",
    "\n",
    "Try and do the following, and make sure you can connect the dots:\n",
    "\n",
    " - Check the `word_index` of your tokenizer\n",
    " - Check the data type of your new training and test sets -- what are they?\n",
    " - What does each document consist of?  What about documents that are less than 150 words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'a': 3,\n",
       " 'of': 4,\n",
       " 'to': 5,\n",
       " 'is': 6,\n",
       " 'br': 7,\n",
       " 'in': 8,\n",
       " 'it': 9,\n",
       " 'i': 10,\n",
       " 'this': 11,\n",
       " 'that': 12,\n",
       " 'was': 13,\n",
       " 'as': 14,\n",
       " 'with': 15,\n",
       " 'for': 16,\n",
       " 'movie': 17,\n",
       " 'but': 18,\n",
       " 'film': 19,\n",
       " 'on': 20,\n",
       " 'not': 21,\n",
       " 'you': 22,\n",
       " 'are': 23,\n",
       " 'his': 24,\n",
       " 'have': 25,\n",
       " 'be': 26,\n",
       " 'one': 27,\n",
       " 'he': 28,\n",
       " 'all': 29,\n",
       " 'at': 30,\n",
       " 'by': 31,\n",
       " 'an': 32,\n",
       " 'they': 33,\n",
       " 'so': 34,\n",
       " 'who': 35,\n",
       " 'from': 36,\n",
       " 'like': 37,\n",
       " 'or': 38,\n",
       " 'just': 39,\n",
       " 'her': 40,\n",
       " 'about': 41,\n",
       " 'out': 42,\n",
       " 'if': 43,\n",
       " \"it's\": 44,\n",
       " 'has': 45,\n",
       " 'some': 46,\n",
       " 'there': 47,\n",
       " 'what': 48,\n",
       " 'good': 49,\n",
       " 'more': 50,\n",
       " 'when': 51,\n",
       " 'very': 52,\n",
       " 'up': 53,\n",
       " 'no': 54,\n",
       " 'even': 55,\n",
       " 'time': 56,\n",
       " 'my': 57,\n",
       " 'she': 58,\n",
       " 'would': 59,\n",
       " 'which': 60,\n",
       " 'only': 61,\n",
       " 'really': 62,\n",
       " 'see': 63,\n",
       " 'story': 64,\n",
       " 'their': 65,\n",
       " 'can': 66,\n",
       " 'had': 67,\n",
       " 'me': 68,\n",
       " 'well': 69,\n",
       " 'were': 70,\n",
       " 'than': 71,\n",
       " 'much': 72,\n",
       " 'we': 73,\n",
       " 'been': 74,\n",
       " 'get': 75,\n",
       " 'bad': 76,\n",
       " 'will': 77,\n",
       " 'also': 78,\n",
       " 'great': 79,\n",
       " 'other': 80,\n",
       " 'do': 81,\n",
       " 'into': 82,\n",
       " 'him': 83,\n",
       " 'people': 84,\n",
       " 'because': 85,\n",
       " 'first': 86,\n",
       " 'how': 87,\n",
       " 'most': 88,\n",
       " \"don't\": 89,\n",
       " 'made': 90,\n",
       " 'them': 91,\n",
       " 'its': 92,\n",
       " 'then': 93,\n",
       " 'make': 94,\n",
       " 'way': 95,\n",
       " 'movies': 96,\n",
       " 'too': 97,\n",
       " 'any': 98,\n",
       " 'could': 99,\n",
       " 'after': 100,\n",
       " 'characters': 101,\n",
       " 'think': 102,\n",
       " 'watch': 103,\n",
       " 'films': 104,\n",
       " 'two': 105,\n",
       " 'seen': 106,\n",
       " 'many': 107,\n",
       " 'character': 108,\n",
       " 'being': 109,\n",
       " 'never': 110,\n",
       " 'love': 111,\n",
       " 'acting': 112,\n",
       " 'plot': 113,\n",
       " 'life': 114,\n",
       " 'best': 115,\n",
       " 'did': 116,\n",
       " 'where': 117,\n",
       " 'little': 118,\n",
       " 'know': 119,\n",
       " 'show': 120,\n",
       " 'over': 121,\n",
       " 'ever': 122,\n",
       " 'off': 123,\n",
       " 'does': 124,\n",
       " 'your': 125,\n",
       " 'better': 126,\n",
       " 'man': 127,\n",
       " 'end': 128,\n",
       " 'scene': 129,\n",
       " 'still': 130,\n",
       " 'these': 131,\n",
       " 'say': 132,\n",
       " 'here': 133,\n",
       " 'scenes': 134,\n",
       " 'why': 135,\n",
       " 'while': 136,\n",
       " 'something': 137,\n",
       " 'such': 138,\n",
       " 'go': 139,\n",
       " 'through': 140,\n",
       " 'back': 141,\n",
       " 'should': 142,\n",
       " 'real': 143,\n",
       " \"i'm\": 144,\n",
       " 'those': 145,\n",
       " 'now': 146,\n",
       " 'thing': 147,\n",
       " 'watching': 148,\n",
       " 'though': 149,\n",
       " \"doesn't\": 150,\n",
       " 'actors': 151,\n",
       " 'years': 152,\n",
       " 'funny': 153,\n",
       " \"didn't\": 154,\n",
       " '10': 155,\n",
       " 'actually': 156,\n",
       " 'another': 157,\n",
       " 'old': 158,\n",
       " 'work': 159,\n",
       " 'before': 160,\n",
       " 'nothing': 161,\n",
       " 'makes': 162,\n",
       " 'director': 163,\n",
       " 'look': 164,\n",
       " 'find': 165,\n",
       " 'new': 166,\n",
       " 'same': 167,\n",
       " 'going': 168,\n",
       " 'lot': 169,\n",
       " 'few': 170,\n",
       " 'again': 171,\n",
       " 'every': 172,\n",
       " 'part': 173,\n",
       " 'down': 174,\n",
       " 'want': 175,\n",
       " 'cast': 176,\n",
       " 'us': 177,\n",
       " 'quite': 178,\n",
       " 'horror': 179,\n",
       " 'things': 180,\n",
       " 'world': 181,\n",
       " 'pretty': 182,\n",
       " 'around': 183,\n",
       " 'seems': 184,\n",
       " 'young': 185,\n",
       " 'take': 186,\n",
       " \"can't\": 187,\n",
       " 'thought': 188,\n",
       " 'however': 189,\n",
       " 'got': 190,\n",
       " 'long': 191,\n",
       " 'enough': 192,\n",
       " 'fact': 193,\n",
       " 'big': 194,\n",
       " 'both': 195,\n",
       " \"that's\": 196,\n",
       " 'give': 197,\n",
       " \"i've\": 198,\n",
       " 'between': 199,\n",
       " 'own': 200,\n",
       " 'may': 201,\n",
       " 'must': 202,\n",
       " 'right': 203,\n",
       " 'comedy': 204,\n",
       " 'music': 205,\n",
       " 'series': 206,\n",
       " 'action': 207,\n",
       " 'times': 208,\n",
       " 'original': 209,\n",
       " 'without': 210,\n",
       " 'always': 211,\n",
       " 'saw': 212,\n",
       " \"isn't\": 213,\n",
       " 'role': 214,\n",
       " 'come': 215,\n",
       " 'gets': 216,\n",
       " 'almost': 217,\n",
       " 'point': 218,\n",
       " 'guy': 219,\n",
       " 'whole': 220,\n",
       " 'interesting': 221,\n",
       " \"there's\": 222,\n",
       " 'done': 223,\n",
       " 'least': 224,\n",
       " 'bit': 225,\n",
       " 'far': 226,\n",
       " 'script': 227,\n",
       " 'making': 228,\n",
       " 'since': 229,\n",
       " 'family': 230,\n",
       " '2': 231,\n",
       " 'minutes': 232,\n",
       " 'feel': 233,\n",
       " 'am': 234,\n",
       " 'might': 235,\n",
       " 'last': 236,\n",
       " 'anything': 237,\n",
       " \"he's\": 238,\n",
       " 'performance': 239,\n",
       " 'kind': 240,\n",
       " 'probably': 241,\n",
       " 'tv': 242,\n",
       " 'away': 243,\n",
       " 'yet': 244,\n",
       " 'worst': 245,\n",
       " 'fun': 246,\n",
       " 'sure': 247,\n",
       " 'rather': 248,\n",
       " 'each': 249,\n",
       " 'anyone': 250,\n",
       " 'found': 251,\n",
       " 'hard': 252,\n",
       " 'played': 253,\n",
       " 'day': 254,\n",
       " 'girl': 255,\n",
       " 'our': 256,\n",
       " 'looking': 257,\n",
       " 'especially': 258,\n",
       " 'woman': 259,\n",
       " 'screen': 260,\n",
       " 'although': 261,\n",
       " 'believe': 262,\n",
       " 'having': 263,\n",
       " 'course': 264,\n",
       " 'dvd': 265,\n",
       " 'trying': 266,\n",
       " 'set': 267,\n",
       " 'book': 268,\n",
       " 'everything': 269,\n",
       " 'goes': 270,\n",
       " 'maybe': 271,\n",
       " 'ending': 272,\n",
       " 'put': 273,\n",
       " 'comes': 274,\n",
       " 'place': 275,\n",
       " 'worth': 276,\n",
       " 'different': 277,\n",
       " 'three': 278,\n",
       " 'once': 279,\n",
       " 'shows': 280,\n",
       " 'sense': 281,\n",
       " 'american': 282,\n",
       " 'main': 283,\n",
       " 'true': 284,\n",
       " 'reason': 285,\n",
       " 'looks': 286,\n",
       " 'money': 287,\n",
       " 'play': 288,\n",
       " 'actor': 289,\n",
       " 'effects': 290,\n",
       " 'together': 291,\n",
       " 'watched': 292,\n",
       " \"wasn't\": 293,\n",
       " 'job': 294,\n",
       " 'someone': 295,\n",
       " 'plays': 296,\n",
       " 'war': 297,\n",
       " 'half': 298,\n",
       " 'instead': 299,\n",
       " 'year': 300,\n",
       " 'high': 301,\n",
       " 'during': 302,\n",
       " 'special': 303,\n",
       " 'everyone': 304,\n",
       " 'said': 305,\n",
       " 'seem': 306,\n",
       " 'beautiful': 307,\n",
       " 'later': 308,\n",
       " 'takes': 309,\n",
       " 'audience': 310,\n",
       " 'john': 311,\n",
       " '1': 312,\n",
       " 'seeing': 313,\n",
       " 'version': 314,\n",
       " 'himself': 315,\n",
       " 'night': 316,\n",
       " 'left': 317,\n",
       " 'excellent': 318,\n",
       " 'black': 319,\n",
       " 'shot': 320,\n",
       " 'house': 321,\n",
       " 'idea': 322,\n",
       " 'mind': 323,\n",
       " 'death': 324,\n",
       " 'wife': 325,\n",
       " 'simply': 326,\n",
       " 'star': 327,\n",
       " 'fan': 328,\n",
       " 'used': 329,\n",
       " 'budget': 330,\n",
       " 'nice': 331,\n",
       " '3': 332,\n",
       " 'completely': 333,\n",
       " \"you're\": 334,\n",
       " 'read': 335,\n",
       " 'poor': 336,\n",
       " 'else': 337,\n",
       " 'short': 338,\n",
       " 'second': 339,\n",
       " 'top': 340,\n",
       " 'along': 341,\n",
       " 'less': 342,\n",
       " 'home': 343,\n",
       " 'help': 344,\n",
       " 'either': 345,\n",
       " 'boring': 346,\n",
       " 'camera': 347,\n",
       " 'men': 348,\n",
       " 'line': 349,\n",
       " 'dead': 350,\n",
       " 'low': 351,\n",
       " 'production': 352,\n",
       " 'kids': 353,\n",
       " 'friends': 354,\n",
       " 'wrong': 355,\n",
       " 'classic': 356,\n",
       " 'use': 357,\n",
       " 'given': 358,\n",
       " 'try': 359,\n",
       " 'enjoy': 360,\n",
       " 'father': 361,\n",
       " 'until': 362,\n",
       " 'truly': 363,\n",
       " 'performances': 364,\n",
       " 'need': 365,\n",
       " 'full': 366,\n",
       " 'school': 367,\n",
       " 'stupid': 368,\n",
       " 'next': 369,\n",
       " 'video': 370,\n",
       " 'start': 371,\n",
       " 'hollywood': 372,\n",
       " 'rest': 373,\n",
       " 'awful': 374,\n",
       " 'couple': 375,\n",
       " 'recommend': 376,\n",
       " 'let': 377,\n",
       " 'sex': 378,\n",
       " 'women': 379,\n",
       " 'tell': 380,\n",
       " 'mean': 381,\n",
       " 'name': 382,\n",
       " 'perhaps': 383,\n",
       " 'remember': 384,\n",
       " 'getting': 385,\n",
       " 'moments': 386,\n",
       " 'came': 387,\n",
       " 'terrible': 388,\n",
       " 'understand': 389,\n",
       " 'itself': 390,\n",
       " 'human': 391,\n",
       " 'face': 392,\n",
       " 'playing': 393,\n",
       " 'wonderful': 394,\n",
       " 'keep': 395,\n",
       " 'doing': 396,\n",
       " 'style': 397,\n",
       " 'small': 398,\n",
       " 'person': 399,\n",
       " 'early': 400,\n",
       " 'episode': 401,\n",
       " 'often': 402,\n",
       " 'perfect': 403,\n",
       " 'others': 404,\n",
       " 'definitely': 405,\n",
       " 'stars': 406,\n",
       " 'written': 407,\n",
       " 'head': 408,\n",
       " 'piece': 409,\n",
       " 'dialogue': 410,\n",
       " 'lines': 411,\n",
       " \"couldn't\": 412,\n",
       " 'gives': 413,\n",
       " 'boy': 414,\n",
       " 'went': 415,\n",
       " 'mother': 416,\n",
       " 'finally': 417,\n",
       " 'absolutely': 418,\n",
       " 'live': 419,\n",
       " 'certainly': 420,\n",
       " 'sort': 421,\n",
       " 'yes': 422,\n",
       " 'oh': 423,\n",
       " 'case': 424,\n",
       " 'become': 425,\n",
       " 'liked': 426,\n",
       " 'title': 427,\n",
       " 'lost': 428,\n",
       " 'worse': 429,\n",
       " 'entertaining': 430,\n",
       " 'laugh': 431,\n",
       " 'loved': 432,\n",
       " 'called': 433,\n",
       " 'mr': 434,\n",
       " 'picture': 435,\n",
       " 'overall': 436,\n",
       " 'hope': 437,\n",
       " 'based': 438,\n",
       " 'guys': 439,\n",
       " 'cinema': 440,\n",
       " 'felt': 441,\n",
       " 'supposed': 442,\n",
       " 'friend': 443,\n",
       " 'drama': 444,\n",
       " 'entire': 445,\n",
       " '5': 446,\n",
       " 'sound': 447,\n",
       " 'against': 448,\n",
       " 'humor': 449,\n",
       " 'waste': 450,\n",
       " '4': 451,\n",
       " 'white': 452,\n",
       " 'problem': 453,\n",
       " 'several': 454,\n",
       " 'dark': 455,\n",
       " 'totally': 456,\n",
       " 'fans': 457,\n",
       " 'care': 458,\n",
       " 'example': 459,\n",
       " 'beginning': 460,\n",
       " 'under': 461,\n",
       " \"she's\": 462,\n",
       " 'lead': 463,\n",
       " 'direction': 464,\n",
       " 'seemed': 465,\n",
       " 'already': 466,\n",
       " 'despite': 467,\n",
       " 'lives': 468,\n",
       " 'wanted': 469,\n",
       " 'final': 470,\n",
       " \"you'll\": 471,\n",
       " 'children': 472,\n",
       " 'guess': 473,\n",
       " 'evil': 474,\n",
       " 'throughout': 475,\n",
       " 'game': 476,\n",
       " 'becomes': 477,\n",
       " 'turn': 478,\n",
       " 'unfortunately': 479,\n",
       " 'able': 480,\n",
       " 'quality': 481,\n",
       " 'amazing': 482,\n",
       " 'side': 483,\n",
       " 'days': 484,\n",
       " \"i'd\": 485,\n",
       " 'fine': 486,\n",
       " 'history': 487,\n",
       " 'heart': 488,\n",
       " 'b': 489,\n",
       " 'horrible': 490,\n",
       " 'close': 491,\n",
       " 'wants': 492,\n",
       " 'writing': 493,\n",
       " 'flick': 494,\n",
       " 'works': 495,\n",
       " 'kill': 496,\n",
       " 'son': 497,\n",
       " 'michael': 498,\n",
       " 'killer': 499,\n",
       " 'run': 500,\n",
       " 'art': 501,\n",
       " 'enjoyed': 502,\n",
       " '\\x96': 503,\n",
       " 'act': 504,\n",
       " 'matter': 505,\n",
       " 'past': 506,\n",
       " 'town': 507,\n",
       " 'etc': 508,\n",
       " 'behind': 509,\n",
       " 'gave': 510,\n",
       " 'parts': 511,\n",
       " \"they're\": 512,\n",
       " \"won't\": 513,\n",
       " 'genre': 514,\n",
       " 'brilliant': 515,\n",
       " 'directed': 516,\n",
       " 'obviously': 517,\n",
       " 'eyes': 518,\n",
       " 'favorite': 519,\n",
       " 'car': 520,\n",
       " 'expect': 521,\n",
       " 'stuff': 522,\n",
       " 'soon': 523,\n",
       " 'hour': 524,\n",
       " 'turns': 525,\n",
       " 'sometimes': 526,\n",
       " 'late': 527,\n",
       " 'tries': 528,\n",
       " 'hand': 529,\n",
       " 'themselves': 530,\n",
       " 'starts': 531,\n",
       " 'self': 532,\n",
       " 'girls': 533,\n",
       " 'viewer': 534,\n",
       " 'child': 535,\n",
       " 'myself': 536,\n",
       " 'killed': 537,\n",
       " 'actress': 538,\n",
       " 'stop': 539,\n",
       " 'city': 540,\n",
       " 'decent': 541,\n",
       " 'voice': 542,\n",
       " 'says': 543,\n",
       " 'highly': 544,\n",
       " 'type': 545,\n",
       " 'anyway': 546,\n",
       " 'blood': 547,\n",
       " 'thinking': 548,\n",
       " 'god': 549,\n",
       " 'feeling': 550,\n",
       " 'took': 551,\n",
       " 'heard': 552,\n",
       " 'group': 553,\n",
       " 'kid': 554,\n",
       " 'known': 555,\n",
       " 'happens': 556,\n",
       " 'except': 557,\n",
       " 'experience': 558,\n",
       " 'coming': 559,\n",
       " 'fight': 560,\n",
       " 'daughter': 561,\n",
       " 'violence': 562,\n",
       " 'slow': 563,\n",
       " 'told': 564,\n",
       " 'stories': 565,\n",
       " 'writer': 566,\n",
       " 'score': 567,\n",
       " 'extremely': 568,\n",
       " 'strong': 569,\n",
       " 'moment': 570,\n",
       " 'leave': 571,\n",
       " 'roles': 572,\n",
       " 'happen': 573,\n",
       " 'wonder': 574,\n",
       " 'lack': 575,\n",
       " 'cannot': 576,\n",
       " 'involved': 577,\n",
       " 'ok': 578,\n",
       " 'age': 579,\n",
       " 'particularly': 580,\n",
       " 'police': 581,\n",
       " 'looked': 582,\n",
       " 'murder': 583,\n",
       " 'chance': 584,\n",
       " 'please': 585,\n",
       " 'cool': 586,\n",
       " 's': 587,\n",
       " 'cut': 588,\n",
       " 'save': 589,\n",
       " 'happened': 590,\n",
       " 'hilarious': 591,\n",
       " 'simple': 592,\n",
       " 'including': 593,\n",
       " 'taken': 594,\n",
       " 'serious': 595,\n",
       " 'complete': 596,\n",
       " 'ago': 597,\n",
       " 'gore': 598,\n",
       " 'crap': 599,\n",
       " 'living': 600,\n",
       " 'attempt': 601,\n",
       " 'hit': 602,\n",
       " 'across': 603,\n",
       " 'obvious': 604,\n",
       " 'james': 605,\n",
       " 'hell': 606,\n",
       " 'song': 607,\n",
       " \"wouldn't\": 608,\n",
       " 'seriously': 609,\n",
       " 'robert': 610,\n",
       " 'david': 611,\n",
       " 'cinematography': 612,\n",
       " 'interest': 613,\n",
       " 'jokes': 614,\n",
       " 'shown': 615,\n",
       " 'reality': 616,\n",
       " 'english': 617,\n",
       " \"film's\": 618,\n",
       " 'none': 619,\n",
       " 'released': 620,\n",
       " 'today': 621,\n",
       " 'opening': 622,\n",
       " 'sad': 623,\n",
       " 'hero': 624,\n",
       " 'brother': 625,\n",
       " 'saying': 626,\n",
       " 'alone': 627,\n",
       " 'talent': 628,\n",
       " 'hours': 629,\n",
       " 'career': 630,\n",
       " 'exactly': 631,\n",
       " 'number': 632,\n",
       " 'usually': 633,\n",
       " 'wish': 634,\n",
       " 'relationship': 635,\n",
       " 'light': 636,\n",
       " 'possible': 637,\n",
       " 'running': 638,\n",
       " 'annoying': 639,\n",
       " 'shots': 640,\n",
       " 'yourself': 641,\n",
       " 'ends': 642,\n",
       " 'view': 643,\n",
       " 'order': 644,\n",
       " 'started': 645,\n",
       " 'husband': 646,\n",
       " 'ridiculous': 647,\n",
       " 'whose': 648,\n",
       " 'huge': 649,\n",
       " 'body': 650,\n",
       " 'taking': 651,\n",
       " 'middle': 652,\n",
       " 'important': 653,\n",
       " 'level': 654,\n",
       " 'scary': 655,\n",
       " 'documentary': 656,\n",
       " 'female': 657,\n",
       " 'opinion': 658,\n",
       " \"i'll\": 659,\n",
       " 'word': 660,\n",
       " 'novel': 661,\n",
       " 'silly': 662,\n",
       " 'turned': 663,\n",
       " 'usual': 664,\n",
       " 'happy': 665,\n",
       " 'major': 666,\n",
       " 'rating': 667,\n",
       " 'mostly': 668,\n",
       " 'knows': 669,\n",
       " 'power': 670,\n",
       " 'four': 671,\n",
       " 'country': 672,\n",
       " 'words': 673,\n",
       " 'disappointed': 674,\n",
       " 'finds': 675,\n",
       " 'ones': 676,\n",
       " 'change': 677,\n",
       " 'apparently': 678,\n",
       " 'knew': 679,\n",
       " 'call': 680,\n",
       " 'beyond': 681,\n",
       " 'somewhat': 682,\n",
       " 'miss': 683,\n",
       " 'non': 684,\n",
       " 'upon': 685,\n",
       " 'cheap': 686,\n",
       " 'talking': 687,\n",
       " 'room': 688,\n",
       " 'modern': 689,\n",
       " 'jack': 690,\n",
       " 'problems': 691,\n",
       " 'strange': 692,\n",
       " 'single': 693,\n",
       " 'attention': 694,\n",
       " 'basically': 695,\n",
       " 'clearly': 696,\n",
       " 'due': 697,\n",
       " 'local': 698,\n",
       " 'class': 699,\n",
       " 'television': 700,\n",
       " '7': 701,\n",
       " 'episodes': 702,\n",
       " 'musical': 703,\n",
       " 'five': 704,\n",
       " 'sequence': 705,\n",
       " 'talk': 706,\n",
       " 'events': 707,\n",
       " \"aren't\": 708,\n",
       " 'earth': 709,\n",
       " 'straight': 710,\n",
       " 'thriller': 711,\n",
       " '8': 712,\n",
       " 'songs': 713,\n",
       " 'british': 714,\n",
       " 'french': 715,\n",
       " 'comic': 716,\n",
       " 'die': 717,\n",
       " 'moving': 718,\n",
       " 'review': 719,\n",
       " 'space': 720,\n",
       " 'tells': 721,\n",
       " 'sets': 722,\n",
       " 'soundtrack': 723,\n",
       " 'fast': 724,\n",
       " 'dialog': 725,\n",
       " 'ten': 726,\n",
       " 'similar': 727,\n",
       " 'team': 728,\n",
       " 'message': 729,\n",
       " 'supporting': 730,\n",
       " 'add': 731,\n",
       " 'future': 732,\n",
       " 'above': 733,\n",
       " 'entertainment': 734,\n",
       " 'falls': 735,\n",
       " 'bring': 736,\n",
       " 'easily': 737,\n",
       " 'whether': 738,\n",
       " 'giving': 739,\n",
       " 'predictable': 740,\n",
       " 'sequel': 741,\n",
       " 'hate': 742,\n",
       " 'near': 743,\n",
       " 'appears': 744,\n",
       " 'suspense': 745,\n",
       " 'showing': 746,\n",
       " 'mention': 747,\n",
       " 'filmed': 748,\n",
       " 'george': 749,\n",
       " 'release': 750,\n",
       " 'lots': 751,\n",
       " 'enjoyable': 752,\n",
       " 'clear': 753,\n",
       " 'romantic': 754,\n",
       " 'tried': 755,\n",
       " 'within': 756,\n",
       " 'dull': 757,\n",
       " 'sorry': 758,\n",
       " 'bunch': 759,\n",
       " 'editing': 760,\n",
       " 'ways': 761,\n",
       " 'easy': 762,\n",
       " 'named': 763,\n",
       " 'eye': 764,\n",
       " 'stay': 765,\n",
       " 'working': 766,\n",
       " \"haven't\": 767,\n",
       " \"'\": 768,\n",
       " 'theme': 769,\n",
       " 'certain': 770,\n",
       " 'among': 771,\n",
       " 'theater': 772,\n",
       " 'needs': 773,\n",
       " 'rock': 774,\n",
       " 'storyline': 775,\n",
       " \"what's\": 776,\n",
       " 'surprised': 777,\n",
       " 'monster': 778,\n",
       " 'gone': 779,\n",
       " 'fall': 780,\n",
       " 'imagine': 781,\n",
       " 'using': 782,\n",
       " 'stand': 783,\n",
       " 'comments': 784,\n",
       " 'parents': 785,\n",
       " 'herself': 786,\n",
       " 'mystery': 787,\n",
       " 'effort': 788,\n",
       " 'feature': 789,\n",
       " 'doubt': 790,\n",
       " 'deal': 791,\n",
       " 'minute': 792,\n",
       " 'kept': 793,\n",
       " 'sister': 794,\n",
       " 'subject': 795,\n",
       " 'dr': 796,\n",
       " 't': 797,\n",
       " 'viewing': 798,\n",
       " 'buy': 799,\n",
       " 'typical': 800,\n",
       " 'elements': 801,\n",
       " 'avoid': 802,\n",
       " '9': 803,\n",
       " 'check': 804,\n",
       " 'okay': 805,\n",
       " 'crime': 806,\n",
       " 'rent': 807,\n",
       " 'tale': 808,\n",
       " 'means': 809,\n",
       " 'brought': 810,\n",
       " 'general': 811,\n",
       " 'nearly': 812,\n",
       " 'king': 813,\n",
       " 'feels': 814,\n",
       " 'fantastic': 815,\n",
       " 'viewers': 816,\n",
       " 'form': 817,\n",
       " 'red': 818,\n",
       " 'famous': 819,\n",
       " 'lady': 820,\n",
       " 'actual': 821,\n",
       " 'richard': 822,\n",
       " 'oscar': 823,\n",
       " 'peter': 824,\n",
       " 'points': 825,\n",
       " 'material': 826,\n",
       " 'realistic': 827,\n",
       " 'forget': 828,\n",
       " 'move': 829,\n",
       " 'somehow': 830,\n",
       " 'reviews': 831,\n",
       " 'dog': 832,\n",
       " 'believable': 833,\n",
       " 'period': 834,\n",
       " 'greatest': 835,\n",
       " 'paul': 836,\n",
       " 'tom': 837,\n",
       " 'open': 838,\n",
       " \"you've\": 839,\n",
       " 'follow': 840,\n",
       " 'learn': 841,\n",
       " 'figure': 842,\n",
       " 'wait': 843,\n",
       " 'atmosphere': 844,\n",
       " 'sequences': 845,\n",
       " 'deep': 846,\n",
       " 'hear': 847,\n",
       " 'begins': 848,\n",
       " 'killing': 849,\n",
       " 'weak': 850,\n",
       " 're': 851,\n",
       " 'eventually': 852,\n",
       " 'box': 853,\n",
       " 'animation': 854,\n",
       " 'dance': 855,\n",
       " 'leads': 856,\n",
       " 'premise': 857,\n",
       " 'whatever': 858,\n",
       " 'surprise': 859,\n",
       " 'note': 860,\n",
       " 'poorly': 861,\n",
       " 'average': 862,\n",
       " 'indeed': 863,\n",
       " 'particular': 864,\n",
       " 'emotional': 865,\n",
       " 'truth': 866,\n",
       " 'sit': 867,\n",
       " 'lame': 868,\n",
       " '20': 869,\n",
       " 'york': 870,\n",
       " 'expected': 871,\n",
       " 'shame': 872,\n",
       " \"who's\": 873,\n",
       " 'sci': 874,\n",
       " 'sexual': 875,\n",
       " 'imdb': 876,\n",
       " 'free': 877,\n",
       " 'fi': 878,\n",
       " 'third': 879,\n",
       " 'possibly': 880,\n",
       " 'romance': 881,\n",
       " 'gay': 882,\n",
       " 'decided': 883,\n",
       " 'otherwise': 884,\n",
       " 'season': 885,\n",
       " 'situation': 886,\n",
       " 'hot': 887,\n",
       " 'meet': 888,\n",
       " 'difficult': 889,\n",
       " 'begin': 890,\n",
       " 'needed': 891,\n",
       " 'hands': 892,\n",
       " 'unless': 893,\n",
       " 'footage': 894,\n",
       " 'leaves': 895,\n",
       " 'question': 896,\n",
       " \"let's\": 897,\n",
       " 'boys': 898,\n",
       " 'write': 899,\n",
       " 'forced': 900,\n",
       " 'crew': 901,\n",
       " 'doctor': 902,\n",
       " 'memorable': 903,\n",
       " 'credits': 904,\n",
       " 'air': 905,\n",
       " 'society': 906,\n",
       " 'acted': 907,\n",
       " 'became': 908,\n",
       " 'reading': 909,\n",
       " 'meets': 910,\n",
       " 'masterpiece': 911,\n",
       " 'badly': 912,\n",
       " 'de': 913,\n",
       " 'features': 914,\n",
       " 'cheesy': 915,\n",
       " 'comment': 916,\n",
       " 'street': 917,\n",
       " 'male': 918,\n",
       " 'previous': 919,\n",
       " 'total': 920,\n",
       " 'beauty': 921,\n",
       " 'plus': 922,\n",
       " 'perfectly': 923,\n",
       " 'stage': 924,\n",
       " 'crazy': 925,\n",
       " 'whom': 926,\n",
       " 'effect': 927,\n",
       " 'setting': 928,\n",
       " 'personal': 929,\n",
       " 'forward': 930,\n",
       " 'e': 931,\n",
       " 'weird': 932,\n",
       " 'sounds': 933,\n",
       " 'japanese': 934,\n",
       " 'western': 935,\n",
       " 'screenplay': 936,\n",
       " 'towards': 937,\n",
       " 'superb': 938,\n",
       " 'laughs': 939,\n",
       " 'nature': 940,\n",
       " 'keeps': 941,\n",
       " 'result': 942,\n",
       " 'background': 943,\n",
       " 'nor': 944,\n",
       " 'mark': 945,\n",
       " 'quickly': 946,\n",
       " 'mess': 947,\n",
       " 'earlier': 948,\n",
       " 'realize': 949,\n",
       " 'writers': 950,\n",
       " 'directing': 951,\n",
       " 'dramatic': 952,\n",
       " 'interested': 953,\n",
       " 'cop': 954,\n",
       " 'inside': 955,\n",
       " 'island': 956,\n",
       " 'america': 957,\n",
       " 'unique': 958,\n",
       " 'directors': 959,\n",
       " 'appear': 960,\n",
       " 'worked': 961,\n",
       " 'copy': 962,\n",
       " 'older': 963,\n",
       " 'political': 964,\n",
       " 'incredibly': 965,\n",
       " 'joke': 966,\n",
       " 'brings': 967,\n",
       " 'creepy': 968,\n",
       " 'dream': 969,\n",
       " 'plenty': 970,\n",
       " 'rate': 971,\n",
       " 'lee': 972,\n",
       " 'rich': 973,\n",
       " 'front': 974,\n",
       " 'following': 975,\n",
       " 'baby': 976,\n",
       " 'bill': 977,\n",
       " 'joe': 978,\n",
       " 'powerful': 979,\n",
       " 'fire': 980,\n",
       " 'battle': 981,\n",
       " 'water': 982,\n",
       " 'dumb': 983,\n",
       " 'apart': 984,\n",
       " 'spent': 985,\n",
       " 'success': 986,\n",
       " 'cover': 987,\n",
       " 'portrayed': 988,\n",
       " 'admit': 989,\n",
       " 'girlfriend': 990,\n",
       " 'leading': 991,\n",
       " 'outside': 992,\n",
       " 'development': 993,\n",
       " 'fairly': 994,\n",
       " 'wasted': 995,\n",
       " 'telling': 996,\n",
       " 'business': 997,\n",
       " 'various': 998,\n",
       " 'deserves': 999,\n",
       " 'present': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your answer here -- the word index tells you what index position each word is at\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4:  Initialize Your Keras Model\n",
    "\n",
    "**What you'll need:** The `Sequential` method from Keras.  This is how you connect different neural network layers together\n",
    "\n",
    "**How it will be setup:** Make it have the following layers:\n",
    "\n",
    " - A word embedding -- make sure the dimensions are as follows:\n",
    "  - `num_words`, `num_weights`, `document_length`\n",
    " - A Dense layer with your choice of neurons and activation function\n",
    " - A Dense layer with **1** neuron and your choice of activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "\n",
    "mod = Sequential([\n",
    "    Embedding(10000, 25, input_length=150),\n",
    "    Flatten(),\n",
    "    Dense(12, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Compile Your Neural Network\n",
    "\n",
    "Unlike scikit-learn, you have to specify a few additional parameters to fit your neural network.  \n",
    "\n",
    "They are as follows:\n",
    "\n",
    " - `optimizer`: this is the technique you use to update your weights.  The standard method is **Stochastic Gradient Descent**, which can be entered as `sgd`.  The more modern method is **ADAM**, which can be entered as `adam`.  Take your pick of which one to choose.\n",
    " - `loss`: this is the loss function you use to **train** your weights.  Since we are doing binary classification then the correct one to use is **binary cross_entropy**\n",
    " - `metrics`: this is how you **score** your model.  This is optional.  But accuracy is always a solid choice here.  This can be entered as `acc`, passed in through a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "mod.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6:  Fit Your Model\n",
    "\n",
    "Now you can go ahead and call fit.  A few arguments to keep in mind:  \n",
    "\n",
    " - `validation_split`: how much of your training data to use for test data.  This takes a decimal less than 1 as an argument.\n",
    " - `epochs`:  how many rounds of training to do to update your weights\n",
    " \n",
    "You can choose the appropriate values for these as you see fit.\n",
    "\n",
    "**Hint:** Keras does not takes pandas as input, so you'll need to make sure it's converted to numpy first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/10\n",
      "32000/32000 [==============================] - 4s 123us/step - loss: 0.3817 - acc: 0.8143 - val_loss: 0.2929 - val_acc: 0.8769\n",
      "Epoch 2/10\n",
      "32000/32000 [==============================] - 4s 117us/step - loss: 0.1497 - acc: 0.9456 - val_loss: 0.3263 - val_acc: 0.8715\n",
      "Epoch 3/10\n",
      "32000/32000 [==============================] - 4s 117us/step - loss: 0.0321 - acc: 0.9928 - val_loss: 0.4146 - val_acc: 0.8686\n",
      "Epoch 4/10\n",
      "32000/32000 [==============================] - 4s 118us/step - loss: 0.0054 - acc: 0.9994 - val_loss: 0.4714 - val_acc: 0.8727\n",
      "Epoch 5/10\n",
      "32000/32000 [==============================] - 4s 118us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.5221 - val_acc: 0.8725\n",
      "Epoch 6/10\n",
      "32000/32000 [==============================] - 4s 116us/step - loss: 3.6469e-04 - acc: 1.0000 - val_loss: 0.5581 - val_acc: 0.8740\n",
      "Epoch 7/10\n",
      "32000/32000 [==============================] - 4s 118us/step - loss: 1.6978e-04 - acc: 1.0000 - val_loss: 0.5899 - val_acc: 0.8744\n",
      "Epoch 8/10\n",
      "32000/32000 [==============================] - 4s 118us/step - loss: 8.9324e-05 - acc: 1.0000 - val_loss: 0.6205 - val_acc: 0.8742\n",
      "Epoch 9/10\n",
      "32000/32000 [==============================] - 4s 117us/step - loss: 4.9566e-05 - acc: 1.0000 - val_loss: 0.6507 - val_acc: 0.8744\n",
      "Epoch 10/10\n",
      "32000/32000 [==============================] - 4s 117us/step - loss: 2.7769e-05 - acc: 1.0000 - val_loss: 0.6817 - val_acc: 0.8746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fdd23f0c358>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your answer here\n",
    "mod.fit(X_train, y_train.values, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Diagnostics\n",
    "\n",
    "Now is a good time to take a look at your results.  \n",
    "\n",
    "By the end of your training run, were you overfitting or underfitting?  Did it look like your results were converging towards a stable answer, or was there more training that needed to be done?  \n",
    "\n",
    "A reasonably good performance on this dataset is a validation accuracy of about 86-89%. \n",
    "\n",
    "If you hit this level, then you should be fine, if you didn't, then you might try changing a few things, including:\n",
    "\n",
    " - Adding more neurons to give your model greater potential for accuracy\n",
    " - Trying a different optimizer (this probably won't help much, but it never hurts)\n",
    " - Using a different set of activation functions\n",
    " - Making your samples longer or shorter in length, or changing the size of the vocabulary\n",
    " \n",
    "Try and fiddle around with a few parameters to see if you can get some measurable improvement.  \n",
    "\n",
    "**Bonus:** The Deep Learning antidote to overfitting is a special type of layeer called **dropout**:  it allows a portion of the data that will be randomly removed between one layer and the next, to prevent a neural network from randomly memorizing spurious connections within your data.\n",
    "\n",
    "It's very easy to setup:\n",
    "\n",
    "`keras.model.layers.Dropout(0.3)`, where `0.3` is the amount of data to randomly remove.  You can add it just like any other layer in your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/10\n",
      "32000/32000 [==============================] - 5s 153us/step - loss: 0.3977 - acc: 0.8063 - val_loss: 0.2789 - val_acc: 0.8834\n",
      "Epoch 2/10\n",
      "32000/32000 [==============================] - 5s 149us/step - loss: 0.1699 - acc: 0.9371 - val_loss: 0.3081 - val_acc: 0.8764\n",
      "Epoch 3/10\n",
      "32000/32000 [==============================] - 5s 146us/step - loss: 0.0577 - acc: 0.9833 - val_loss: 0.3938 - val_acc: 0.8700\n",
      "Epoch 4/10\n",
      "32000/32000 [==============================] - 5s 149us/step - loss: 0.0207 - acc: 0.9949 - val_loss: 0.4806 - val_acc: 0.8679\n",
      "Epoch 5/10\n",
      "32000/32000 [==============================] - 5s 149us/step - loss: 0.0124 - acc: 0.9963 - val_loss: 0.5690 - val_acc: 0.8709\n",
      "Epoch 6/10\n",
      "32000/32000 [==============================] - 5s 149us/step - loss: 0.0103 - acc: 0.9971 - val_loss: 0.6440 - val_acc: 0.8646\n",
      "Epoch 7/10\n",
      "32000/32000 [==============================] - 5s 150us/step - loss: 0.0146 - acc: 0.9948 - val_loss: 0.7443 - val_acc: 0.8625\n",
      "Epoch 8/10\n",
      "32000/32000 [==============================] - 5s 146us/step - loss: 0.0149 - acc: 0.9944 - val_loss: 0.7291 - val_acc: 0.8644\n",
      "Epoch 9/10\n",
      "32000/32000 [==============================] - 5s 149us/step - loss: 0.0141 - acc: 0.9948 - val_loss: 0.7540 - val_acc: 0.8624\n",
      "Epoch 10/10\n",
      "32000/32000 [==============================] - 5s 146us/step - loss: 0.0121 - acc: 0.9958 - val_loss: 0.7770 - val_acc: 0.8601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fdd238460b8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the model began overfitting @ epoch 3, so we'll add a dropout layer of 0.2\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# build\n",
    "mod = Sequential([\n",
    "    Embedding(10000, 25, input_length=150),\n",
    "    Flatten(),\n",
    "    Dropout(0.2),\n",
    "    Dense(12, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile\n",
    "mod.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# fit\n",
    "mod.fit(X_train, y_train.values, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 5s 149us/step - loss: 0.0098 - acc: 0.9962 - val_loss: 0.8150 - val_acc: 0.8644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fdd23e57ac8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we had the best validation performance after one round, we'll go ahead and just train it for that much\n",
    "mod.fit(X_train, y_train.values, validation_split=0.2, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 24us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.886080420306325, 0.8533999919891357]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and finally score on our test set\n",
    "mod.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
